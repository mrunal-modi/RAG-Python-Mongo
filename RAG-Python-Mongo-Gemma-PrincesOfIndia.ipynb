{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23316fc4",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32f7cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install datasets pandas pymongo sentence_transformers\n",
    "# pip install -U transformers\n",
    "# huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5a25a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "from datasets import load_dataset\n",
    "from configparser import ConfigParser\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255dc62",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c02879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = '_credentials.conf'\n",
    "config = ConfigParser()\n",
    "config.read(file)\n",
    "mongo_username = config['mongo_atlas_princesofindia']['username']\n",
    "mongo_password = config['mongo_atlas_princesofindia']['password']\n",
    "huggingFaceAccess_token = config['huggingFace']['token']\n",
    "mongo_uri = f'mongodb+srv://{mongo_username}:{mongo_password}@princesofindia.vb2f8zo.mongodb.net/?retryWrites=true&w=majority&appName=princesofindia' \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token=huggingFaceAccess_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0916006",
   "metadata": {},
   "source": [
    "## Step1: Load DATA | Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750d118c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset to a pandas DataFrame\n",
    "dataset_df = pd.read_csv(\"https://raw.githubusercontent.com/mrunal-modi/princesofindia-data/main/v2/_persons_v2.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75fbabc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedding models convert high-dimensional data such as text, audio, and images into a \n",
    "# lower-dimensional numerical representation that captures the input data's semantics and context. \n",
    "# This embedding representation of data can be used to conduct semantic searches \n",
    "# based on the positions and proximity of embeddings to each other within a vector space.\n",
    "# The embedding model used in the RAG system is the Generate Text Embedding (GTE) model, based on the BERT model. \n",
    "# The GTE embedding models come in three variants, mentioned below, \n",
    "# and were trained and released by Alibaba DAMO Academy, a research institution.\n",
    "# https://huggingface.co/thenlper/gte-large\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3883cffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    if not text.strip():\n",
    "        print(\"Attempted to get embedding for empty text.\")\n",
    "        return []\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf08529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_df[\"embedding\"] = dataset_df[\"bio\"].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e8c91",
   "metadata": {},
   "source": [
    "## Step2: Ingest DATA | Create Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31661266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mongo_client(mongo_uri):\n",
    "    \"\"\"Establish connection to the MongoDB.\"\"\"\n",
    "    try:\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        print(\"Connection to MongoDB successful\")\n",
    "        return client\n",
    "    except pymongo.errors.ConnectionFailure as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1586c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect + Create MongoDB collection\n",
    "mongo_client = get_mongo_client(mongo_uri)\n",
    "db = mongo_client[\"princesofindia\"]\n",
    "collection = db[\"princesofindia_collection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489444b-e67f-4d1d-a291-f020cbf465ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete any existing records in the collection\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96681c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ingest data into MongoDB\n",
    "documents = dataset_df.to_dict('records')\n",
    "collection.insert_many(documents)\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091336fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vector search index creation using Mongo Atlas > Atlas Search > JSON Editor\n",
    "\n",
    "{\n",
    " \"fields\": [{\n",
    "     \"numDimensions\": 1024,\n",
    "     \"path\": \"embedding\",\n",
    "     \"similarity\": \"cosine\",\n",
    "     \"type\": \"vector\"\n",
    "   }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcaddca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vector_search(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(user_query)\n",
    "    \n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
    "                \"limit\": 100,  # Return top 4 matches\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  # Exclude the _id field\n",
    "                \"name\": 1,  # Include the name field\n",
    "                \"bio\": 1,  # Include the bio field\n",
    "                \"region\": 1,  # Include the region field\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"},  # Include the search score\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f95beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_search_result(query, collection):\n",
    "    get_knowledge = vector_search(query, collection)\n",
    "    search_result = \"\"\n",
    "    for result in get_knowledge:\n",
    "        search_result += f\"Name: {result.get('name', 'N/A')}, Plot: {result.get('bio', 'N/A')}\\n\"\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b2c68",
   "metadata": {},
   "source": [
    "## Step3: Create user queries (Grounding) > Passing to Gemma (LLM) > Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9cffa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A user query is defined in the code snippet above; \n",
    "# this query is the target for semantic search against the movie embeddings in the database collection. \n",
    "# The query and vector search results are combined into a single string to pass as a full context \n",
    "# to the base model for the RAG system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b4386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conduct query with retrieval of sources\n",
    "query = \"Which Prince is the richest and from which state?\"\n",
    "source_information = get_search_result(query, collection)\n",
    "combined_information = f\"Query: {query}\\nContinue to answer the query by using the Search Results:\\n{source_information}.\"\n",
    "print(combined_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419bb6e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CPU Enabled uncomment below\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "# # GPU Enabled use below\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a43dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer(combined_information, return_tensors=\"pt\")\n",
    "response = model.generate(**input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(response[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b448cca-91da-42df-b259-a3292e15b3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
